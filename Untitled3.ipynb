{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1590c6c-b201-46bb-bebf-833f34804878",
   "metadata": {},
   "source": [
    "### Importing Required Libraries\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "**transformers.pipeline** → From Hugging Face 🤗, this provides a simple interface to use pretrained models (e.g., for text generation, question answering, summarization). We’ll use it to load a language model that can generate answers.\n",
    "\n",
    "**PromptTemplate** → Lets us define reusable prompt structures. Instead of hardcoding text prompts, we use templates where we can inject context and questions.\n",
    "\n",
    "**load_qa_chain** → Utility from LangChain to build a Question-Answering chain. It connects our model + retriever + prompt into a single pipeline.\n",
    "\n",
    "**PyPDFLoader** → Helps us load text directly from PDF documents, so we can later split and process them for question answering.\n",
    "\n",
    "**RecursiveCharacterTextSplitter** → Splits long documents into smaller chunks (e.g., 500–1000 tokens). This is important because models cannot process huge documents in one go, so we break them into overlapping pieces.\n",
    "\n",
    "**Chroma** → A vector database that stores embeddings of document chunks. This allows us to efficiently retrieve the most relevant parts of a document when a user asks a question.\n",
    "\n",
    "**HuggingFaceEmbeddings** → Converts text into numerical vectors (embeddings) using pretrained Hugging Face models. These vectors are used to store and search text in Chroma.\n",
    "\n",
    "Why this step?\n",
    "\n",
    "This sets up all the building blocks for our PDF-based Question Answering system:\n",
    "\n",
    "Load documents (PDF).\n",
    "\n",
    "Split into chunks.\n",
    "\n",
    "Convert chunks into embeddings.\n",
    "\n",
    "Store in vector DB (Chroma).\n",
    "\n",
    "Use retriever + model for QA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b310cf36-2943-429f-bba6-454f8ad0c123",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9684320-e2ed-431f-8602-6fde45b5dfc4",
   "metadata": {},
   "source": [
    "### Loading PDF Document\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "PyPDFLoader(\"gen ai resume 3.pdf\") → Initializes a loader to read the content from the given PDF file. Here, the file name is \"gen ai resume 3.pdf\".\n",
    "\n",
    "loader.load() → Actually extracts the text from the PDF and stores it in a structured format as a list of Document objects.\n",
    "\n",
    "Each Document contains:\n",
    "\n",
    "page_content: the text content of that page.\n",
    "\n",
    "metadata: information like page number.\n",
    "\n",
    "**Why this step?**\n",
    "\n",
    "We need the raw text from the PDF before we can process it further.\n",
    "\n",
    "Storing it as Document objects makes it easier to handle later when splitting into smaller chunks and creating embeddings.\n",
    "\n",
    "At this stage, if you print document[:1], you’ll see the first page’s text and metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "dc908f69-781c-4a53-b49f-b0878b6adc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(\"gen ai resume 3.pdf\")\n",
    "\n",
    "document = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ef2edd2e-9ae9-4e7d-aba8-68134910212c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2025-08-07T13:29:44+05:30', 'author': 'python-docx', 'moddate': '2025-08-07T13:29:44+05:30', 'source': 'gen ai resume 3.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1'}, page_content='Rushikesh Panjabrao Chavan  \\nrishichavan462@gmail.com | +91 7057606243 | Pune | LinkedIn | GitHub  \\n  \\nObjective \\nAspiring Data Scientist with a strong foundation in statistics, Python, and machine learning, along with practical \\nexperience in deep learning, SQL, and Generative AI. Passionate about leveraging data to develop innovative, real-world \\nsolutions to complex problems. Skilled in applying ML/DL algorithms, building end-to-end pipelines, and deploying \\nsolutions using tools like Pandas, Scikit-learn, TensorFlow, Hugging Face Transformers, and Power BI. \\nHands-on experience with Gen AI projects, including text summarization, document Q&A, and prompt engineering using \\nOpenAI, LangChain, and LLMs. Eager to contribute technical expertise and impactful projects in a collaborative, growth -\\noriented environment. \\n \\nTechnical Skills  \\nProgramming & Query Languages: Python, SQL \\nMachine Learning & Deep Learning: Supervised & Unsupervised Learning, CNN, RNN, LSTM, GRU \\nGen AI & LLMs: LangChain, Hugging Face, Transformers, RAG, Prompt Engineering, Gemini, OpenAI  \\nNatural Language Processing (NLP): Sentiment Analysis, TF-IDF, Word2Vec, Document QA \\nLibraries & Frameworks: Scikit-learn, TensorFlow, Keras, Pandas, NumPy, Matplotlib, Seaborn \\nComputer Vision: OpenCV, MediaPipe, Image Processing \\nDeployment & Tools: Streamlit, Flask, AWS, PySpark, FAISS, Git, Power BI, Google Colab, Jupyter \\nSoft Skills: Problem Solving, Critical Thinking, Collaboration, Communication, Adaptability \\n \\nExperience \\nData Science Intern  \\nInnomatics Research Labs                                                                        \\nFeb 2025 – June 2025 \\n \\nEducation  \\nB.Sc. Physics — Bharati Vidyapeeth, Pune                                                                                                         2018 – 2021  \\n \\nProjects  \\nStress Level Prediction using Random Forest & Streamlit \\nMachine Learning | Scikit-learn, Streamlit, Pandas \\n• Built a machine learning model to predict stress levels (High, Medium, Low) using lifestyle, health, and \\ndemographic features like sleep duration, cholesterol level, and meditation habits.  \\n• Performed data preprocessing, feature encoding, and scaling; selected the Random Forest Classifier based on \\nperformance metrics including accuracy, precision, and recall to ensure robust predictions. \\n• Developed an interactive Streamlit web app with two modules: (1) EDA Dashboard for dataset exploration and \\ninsights, and (2) Stress Detection App for real-time predictions and personalized recommendations. \\n \\nDocument Question Answering System with RAG and Hugging Face \\nNLP | Streamlit, LangChain, Hugging Face, FAISS, Python \\n• Built an intelligent Document Question Answering (QA) system leveraging Retrieval-Augmented Generation \\n(RAG) to extract context-aware answers from PDF documents.'), Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2025-08-07T13:29:44+05:30', 'author': 'python-docx', 'moddate': '2025-08-07T13:29:44+05:30', 'source': 'gen ai resume 3.pdf', 'total_pages': 2, 'page': 1, 'page_label': '2'}, page_content='• Integrated LangChain for orchestrating retrieval and generation workflows, FAISS for efficient vector similarity \\nsearch, and Hugging Face Transformers for lightweight yet effective text generation. \\n• Designed and implemented a Streamlit interface enabling users to upload PDFs and interact with the content \\nthrough natural language queries.  \\n  \\n \\nRice Grain Classification Using CNN \\nDeep Learning | Python, TensorFlow/Keras \\n• Developed a Convolutional Neural Network to classify rice grains into 5 varieties with ~96% validation accuracy. \\n• Performed image preprocessing (resizing, normalization, grayscale conversion) and trained the model on 75,000 \\nimages.  \\n• Utilized OpenCV for image preprocessing, including resizing and grayscale conversion, to prepare 75,000 rice \\ngrain images for CNN training. \\n \\nSentiment Analysis on Flipkart Product Reviews using ML, RNN, LSTM, GRU \\nNLP | Scikit-learn, TensorFlow, Keras, Streamlit \\n• Built an end-to-end sentiment analysis system to classify Flipkart product reviews (YONEX MAVIS 350) as \\npositive or negative using traditional ML models and deep learning (Simple RNN, LSTM, GRU).  \\n• Performed text preprocessing (lemmatization, stopword removal), feature extraction (TF -IDF, Word2Vec), and \\ntrained multiple models; LSTM achieved highest F1-score and accuracy. \\n• Developed an interactive Streamlit app for real-time sentiment detection with user input, model inference, and \\nvisualization. \\n•  \\nPublications  \\n\"A Beginner’s Guide to Language Modeling: From N-grams to Transformers\" \\nPublished on Medium \\nExplained the fundamentals and evolution of language models, from statistical approaches like n -grams to advanced \\ntransformer-based architectures like GPT and BERT. Covered real-world applications, challenges like bias and \\nhallucination, and visual intuitions for NLP learners. \\n  \\nExtracurricular  \\nOpen-Source Contributor – GitHub (2025 – Present)  \\n• Contributed to open-source ML, DL, GEN AI projects including custom Streamlit apps and improvements to scikit -\\nlearn tools.  \\n• Active in issue resolution, code reviews, and documentation.  \\n•  \\nCertifications  \\nAdvanced Data Science with Python — NASSCOM FutureSkills Prime | April 2025')]\n"
     ]
    }
   ],
   "source": [
    "print(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8313500c-7ed6-4763-a8fd-f695f232c333",
   "metadata": {},
   "source": [
    "### Splitting PDF into Chunks\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "RecursiveCharacterTextSplitter → A utility that breaks large documents into smaller, manageable chunks. This is important because:\n",
    "\n",
    "LLMs (like Flan-T5, GPT, etc.) have input size limits (context window).\n",
    "\n",
    "Smaller chunks ensure no important content gets cut off.\n",
    "\n",
    "Parameters used:\n",
    "\n",
    "chunk_size=1000 → Each chunk will contain up to 1000 characters of text.\n",
    "\n",
    "chunk_overlap=200 → Each chunk will overlap with the previous one by 200 characters.\n",
    "\n",
    "Helps preserve context between chunks (so information isn’t lost at boundaries).\n",
    "\n",
    "split_documents(document) → Takes the list of PDF pages (document) and splits them into smaller chunks, returning a new list docs (each item is still a Document object but smaller in size).\n",
    "\n",
    "**Why this step?**\n",
    "\n",
    "Improves retrieval quality (retrievers can match smaller text pieces more precisely).\n",
    "\n",
    "Prevents exceeding token limits when passing content to LLMs.\n",
    "\n",
    "At this stage, if you print len(docs), you’ll see how many chunks were created. If you do docs[0].page_content, you’ll see the first chunk’s text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "abc610e9-b033-458b-a21f-1f592aa35e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "\n",
    "docs = text_splitter.split_documents(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "85f4b82d-cb0a-479e-8df8-3f9145972633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f657736-422a-447b-beb2-ad0d453d270d",
   "metadata": {},
   "source": [
    "### Creating Embeddings from Text\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "What are embeddings?\n",
    "Embeddings are numerical vector representations of text. They capture the semantic meaning of sentences/paragraphs so that similar texts have vectors that are close together in a high-dimensional space.\n",
    "\n",
    "Why do we need embeddings?\n",
    "\n",
    "To make the text searchable & comparable.\n",
    "\n",
    "Instead of keyword matching, embeddings allow semantic similarity search → e.g., “What is activation function?” will find content even if the PDF says “activation functions decide neuron firing.”\n",
    "\n",
    "Model used:\n",
    "\n",
    "\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "A lightweight but powerful model for sentence-level embeddings.\n",
    "\n",
    "Produces 384-dimensional vectors.\n",
    "\n",
    "Trade-off: small model = faster, good quality, efficient for local use.\n",
    "\n",
    "HuggingFaceEmbeddings → LangChain wrapper that makes it easy to call HuggingFace embedding models.\n",
    "\n",
    "**Why this step?**\n",
    "\n",
    "This is the foundation of semantic search. Later, we’ll store these embeddings in a vector database (Chroma) and query them when a user asks questions.\n",
    "\n",
    "After this step, you don’t yet see vectors created. They’ll be generated when we pass the document chunks into Chroma in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f37cdb7b-5888-44c2-9534-d65dcf5b1408",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5166a489-6c0e-47bb-8d5c-e16ae3c68b03",
   "metadata": {},
   "source": [
    "### Creating a Vector Store with Chroma\n",
    "\n",
    "**What is a Vector Store?**\n",
    "\n",
    "A database optimized for embeddings.\n",
    "\n",
    "It stores document chunks as vectors and allows fast similarity search (finding the most relevant chunks for a given query).\n",
    "\n",
    "**Why Chroma?**\n",
    "\n",
    "Open-source, lightweight, and easy to use.\n",
    "\n",
    "Integrates smoothly with LangChain.\n",
    "\n",
    "Supports persistent storage → you don’t need to recompute embeddings every time; they’re saved on disk.\n",
    "\n",
    "Breaking down the code:\n",
    "\n",
    "Chroma.from_documents(docs, embedding_model, ...)\n",
    "\n",
    "Takes your split documents (docs).\n",
    "\n",
    "Converts them into embeddings using the embedding_model.\n",
    "\n",
    "Stores those embeddings + text inside a Chroma database.\n",
    "\n",
    "persist_directory='chroma_store'\n",
    "\n",
    "Saves the vector store locally in a folder named chroma_store.\n",
    "\n",
    "Next time you can just reload instead of re-processing the PDF.\n",
    "\n",
    "**Why this step?**\n",
    "\n",
    "Without a vector store, we’d have to scan through the whole document manually.\n",
    "\n",
    "Now, when a user asks a question, we can retrieve only the most relevant chunks instead of passing the entire document to the model.\n",
    "\n",
    "This makes the QA system efficient, scalable, and accurate.\n",
    "\n",
    "After this step, your PDF is now searchable by meaning (semantics), not just exact words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fe8c13c9-505f-4eb6-ac8f-ffaec0cabc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = Chroma.from_documents(docs, embedding_model, persist_directory='chroma_store')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1204a927-bb3d-4dc6-802a-7732bff3833b",
   "metadata": {},
   "source": [
    "### Creating a Retriever from the Vector Store\n",
    "\n",
    "**What is a Retriever?**\n",
    "\n",
    "A retriever is a tool that fetches the most relevant chunks from your vector store when you ask a question.\n",
    "\n",
    "It uses semantic similarity search: instead of matching exact words, it compares the meaning of your query with the stored document embeddings.\n",
    "\n",
    "Breaking down the code:\n",
    "\n",
    "vector_store.as_retriever() → converts the Chroma vector store into a retriever object.\n",
    "\n",
    "search_kwargs={'k':3} → tells the retriever to return the top 3 most relevant document chunks for any given query.\n",
    "\n",
    "**Why do we need this?**\n",
    "\n",
    "Large documents are split into many small chunks.\n",
    "\n",
    "Instead of sending the entire PDF to the model (inefficient + costly), we only fetch the 3 best-matching chunks.\n",
    "\n",
    "This makes the QA system:\n",
    "\n",
    "Faster → less data to process.\n",
    "\n",
    "Cheaper → fewer tokens used.\n",
    "\n",
    "More accurate → model focuses only on the relevant context.\n",
    "\n",
    "**Analogy:**\n",
    "Think of the retriever as a smart librarian. Instead of giving you the whole library, they hand-pick the 3 most useful books/pages for your question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "aefc98f9-6c33-412e-92c8-cc9281224dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever(search_kwargs={'k':3})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80008a54-1480-4e06-9d81-76e89039c9c2",
   "metadata": {},
   "source": [
    "### Writing a query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "32c4c437-db7e-49c7-b3a4-74a8b9e582a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"what is the experience this person has\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58bedca-2505-4cd9-9d61-b7a6172caaae",
   "metadata": {},
   "source": [
    "### Retrieving Relevant Documents with a Query\n",
    "\n",
    "What this does:\n",
    "\n",
    "Sends your input query (a user’s question) to the retriever.\n",
    "\n",
    "The retriever compares the embedding of the query with the embeddings of all stored document chunks in the vector store.\n",
    "\n",
    "It returns the most relevant chunks (in our case, top k=3 chunks).\n",
    "\n",
    "Breaking it down:\n",
    "\n",
    "query → the user’s question (e.g., \"What is an activation function?\").\n",
    "\n",
    "retriever.get_relevant_documents(query) → retrieves the 3 most semantically similar chunks.\n",
    "\n",
    "result → a list of Document objects, each containing:\n",
    "\n",
    ".page_content → the text of that chunk.\n",
    "\n",
    ".metadata → extra info like page number.\n",
    "\n",
    "**Why do we need this?**\n",
    "\n",
    "Instead of passing the entire PDF, we now only pass the most relevant chunks to the LLM.\n",
    "\n",
    "This ensures the model stays focused on contextually correct information and avoids hallucinations.\n",
    "\n",
    "**Analogy:**\n",
    "\n",
    "Imagine asking a librarian \"Tell me about activation functions\". \n",
    "\n",
    "Instead of giving you the whole book, they open the 3 most relevant pages and hand them to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1552e6ac-cb40-465d-8a29-eb10a1e4f0dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manisha\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "result = retriever.get_relevant_documents(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f3a50211-1ace-4cf3-8d73-7a1e1c22557f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'creator': 'Microsoft® Word 2019', 'moddate': '2025-08-07T13:29:44+05:30', 'author': 'python-docx', 'page_label': '1', 'total_pages': 2, 'creationdate': '2025-08-07T13:29:44+05:30', 'page': 0, 'producer': 'Microsoft® Word 2019', 'source': 'gen ai resume 3.pdf'}, page_content='Experience \\nData Science Intern  \\nInnomatics Research Labs                                                                        \\nFeb 2025 – June 2025 \\n \\nEducation  \\nB.Sc. Physics — Bharati Vidyapeeth, Pune                                                                                                         2018 – 2021  \\n \\nProjects  \\nStress Level Prediction using Random Forest & Streamlit \\nMachine Learning | Scikit-learn, Streamlit, Pandas \\n• Built a machine learning model to predict stress levels (High, Medium, Low) using lifestyle, health, and \\ndemographic features like sleep duration, cholesterol level, and meditation habits.  \\n• Performed data preprocessing, feature encoding, and scaling; selected the Random Forest Classifier based on \\nperformance metrics including accuracy, precision, and recall to ensure robust predictions. \\n• Developed an interactive Streamlit web app with two modules: (1) EDA Dashboard for dataset exploration and'), Document(metadata={'producer': 'Microsoft® Word 2019', 'source': 'gen ai resume 3.pdf', 'author': 'python-docx', 'total_pages': 2, 'moddate': '2025-08-07T13:29:44+05:30', 'creator': 'Microsoft® Word 2019', 'creationdate': '2025-08-07T13:29:44+05:30', 'page_label': '1', 'page': 0}, page_content='Experience \\nData Science Intern  \\nInnomatics Research Labs                                                                        \\nFeb 2025 – June 2025 \\n \\nEducation  \\nB.Sc. Physics — Bharati Vidyapeeth, Pune                                                                                                         2018 – 2021  \\n \\nProjects  \\nStress Level Prediction using Random Forest & Streamlit \\nMachine Learning | Scikit-learn, Streamlit, Pandas \\n• Built a machine learning model to predict stress levels (High, Medium, Low) using lifestyle, health, and \\ndemographic features like sleep duration, cholesterol level, and meditation habits.  \\n• Performed data preprocessing, feature encoding, and scaling; selected the Random Forest Classifier based on \\nperformance metrics including accuracy, precision, and recall to ensure robust predictions. \\n• Developed an interactive Streamlit web app with two modules: (1) EDA Dashboard for dataset exploration and'), Document(metadata={'total_pages': 2, 'creationdate': '2025-08-07T13:29:44+05:30', 'moddate': '2025-08-07T13:29:44+05:30', 'producer': 'Microsoft® Word 2019', 'author': 'python-docx', 'page_label': '1', 'page': 0, 'source': 'gen ai resume 3.pdf', 'creator': 'Microsoft® Word 2019'}, page_content='• Developed an interactive Streamlit web app with two modules: (1) EDA Dashboard for dataset exploration and \\ninsights, and (2) Stress Detection App for real-time predictions and personalized recommendations. \\n \\nDocument Question Answering System with RAG and Hugging Face \\nNLP | Streamlit, LangChain, Hugging Face, FAISS, Python \\n• Built an intelligent Document Question Answering (QA) system leveraging Retrieval-Augmented Generation \\n(RAG) to extract context-aware answers from PDF documents.')]\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68b8310-43fd-4ac4-8f60-3264746a597a",
   "metadata": {},
   "source": [
    "### Inspecting Retrieved Documents\n",
    "\n",
    "Iterates over the retrieved documents (result).\n",
    "\n",
    "Prints the first 300 characters of each document’s text (doc.page_content[:300]).\n",
    "\n",
    "Adds a numbered header (--- Result 1 ---, --- Result 2 ---, etc.) for clarity.\n",
    "\n",
    "Breaking it down:\n",
    "\n",
    "enumerate(result, 1) → loops through the results, starting count at 1 instead of 0.\n",
    "\n",
    "doc.page_content → contains the actual text content of that chunk.\n",
    "\n",
    "[:300] → shows only the first 300 characters to avoid overwhelming output.\n",
    "\n",
    "Why do we need this?\n",
    "\n",
    "This step is mainly for debugging and verification.\n",
    "\n",
    "It lets us peek at what the retriever returned before passing it to the LLM.\n",
    "\n",
    "Helps ensure the retrieved context is relevant and accurate for answering the query.\n",
    "\n",
    "Analogy:\n",
    "Think of it as skimming the first few sentences of the pages the librarian gave you, to quickly check if they’re indeed about activation functions before you start reading in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "74d92a66-7d43-46df-ba4b-43df86a1584c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Result 1 ---\n",
      "Experience \n",
      "Data Science Intern  \n",
      "Innomatics Research Labs                                                                        \n",
      "Feb 2025 – June 2025 \n",
      " \n",
      "Education  \n",
      "B.Sc. Physics — Bharati Vidyapeeth, Pune                                                                                             \n",
      "--- Result 2 ---\n",
      "Experience \n",
      "Data Science Intern  \n",
      "Innomatics Research Labs                                                                        \n",
      "Feb 2025 – June 2025 \n",
      " \n",
      "Education  \n",
      "B.Sc. Physics — Bharati Vidyapeeth, Pune                                                                                             \n",
      "--- Result 3 ---\n",
      "• Developed an interactive Streamlit web app with two modules: (1) EDA Dashboard for dataset exploration and \n",
      "insights, and (2) Stress Detection App for real-time predictions and personalized recommendations. \n",
      " \n",
      "Document Question Answering System with RAG and Hugging Face \n",
      "NLP | Streamlit, LangChain\n"
     ]
    }
   ],
   "source": [
    "for i, doc in enumerate(result,1):\n",
    "    print(f\"--- Result {i} ---\")\n",
    "    print(doc.page_content[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ba2009a3-9fe1-43b6-aba9-e5bf03eeb284",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFacePipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4ef856f8-dbef-46fc-a1c0-9330a73fc883",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "generate_pipeline = pipeline('text2text-generation', model = 'google/flan-t5-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e84de32e-c5e9-4c6e-b6ca-0602701cb3fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manisha\\AppData\\Local\\Temp\\ipykernel_4656\\3060906120.py:1: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  llm = HuggingFacePipeline(pipeline=generate_pipeline)\n"
     ]
    }
   ],
   "source": [
    "llm = HuggingFacePipeline(pipeline=generate_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d7699894-d250-451b-82b2-9e206649ee4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manisha\\AppData\\Local\\Temp\\ipykernel_4656\\1538851652.py:1: LangChainDeprecationWarning: This class is deprecated. See the following migration guides for replacements based on `chain_type`:\n",
      "stuff: https://python.langchain.com/docs/versions/migrating_chains/stuff_docs_chain\n",
      "map_reduce: https://python.langchain.com/docs/versions/migrating_chains/map_reduce_chain\n",
      "refine: https://python.langchain.com/docs/versions/migrating_chains/refine_chain\n",
      "map_rerank: https://python.langchain.com/docs/versions/migrating_chains/map_rerank_docs_chain\n",
      "\n",
      "See also guides on retrieval and question-answering here: https://python.langchain.com/docs/how_to/#qa-with-rag\n",
      "  qa_chain = load_qa_chain(llm, chain_type = 'stuff')\n"
     ]
    }
   ],
   "source": [
    "qa_chain = load_qa_chain(llm, chain_type = 'stuff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "4bc661aa-4893-4655-b7aa-bf7a6e27c48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"what is the experience this person has?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "860b36dd-33d3-493a-bd3d-05f3f1b66724",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manisha\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "retrieved_docs = retriever.get_relevant_documents(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "33cababb-4a8d-4fcb-8785-8b7e67f4aefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = qa_chain.run(input_documents=retrieved_docs, question=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f7cbd8b0-9d88-497e-a7a2-a37aa92ca2d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer :\n",
      " Data Science Intern Innomatics Research Labs Feb 2025 – June 2025 Education B.Sc. Physics — Bharati Vidyapeeth, Pune 2018 – 2021 Projects Stress Level Prediction using Random Forest & Streamlit Machine Learning | Scikit-learn, Streamlit, Pandas\n"
     ]
    }
   ],
   "source": [
    "print(\"answer :\\n\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dee0a10-6f3d-443f-a5c9-956af5e3e7c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "16f4343b-dadf-4272-8748-553a6fd32704",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "In this workflow, we successfully built a Retrieval-Augmented Generation (RAG) pipeline using LangChain, Hugging Face models, and ChromaDB. Let’s recap the main components and why each was important:\n",
    "\n",
    "**Document Loading**\n",
    "\n",
    "We used PyPDFLoader to load the PDF file into LangChain as structured Document objects.\n",
    "\n",
    "This gave us an easy way to extract raw text while keeping metadata.\n",
    "\n",
    "**Text Splitting**\n",
    "\n",
    "Applied RecursiveCharacterTextSplitter to break the document into smaller, overlapping chunks.\n",
    "\n",
    "Chunking ensures we don’t lose context while also making retrieval efficient and avoiding model input size limits.\n",
    "\n",
    "**Embedding Generation**\n",
    "\n",
    "Used HuggingFaceEmbeddings (all-MiniLM-L6-v2) to convert text chunks into vector representations.\n",
    "\n",
    "Embeddings are the semantic fingerprints of text, making it possible to compare meaning, not just keywords.\n",
    "\n",
    "**Vector Store (ChromaDB)**\n",
    "\n",
    "Stored the embeddings inside a local Chroma database (persist_directory='chroma_store').\n",
    "\n",
    "This allows fast similarity search, so we can retrieve the most relevant chunks for a query.\n",
    "\n",
    "**Retriever**\n",
    "\n",
    "Converted the vector store into a retriever that fetches top-k relevant chunks.\n",
    "\n",
    "This is the “retrieval” part of RAG — instead of making the LLM guess from memory, we ground it with external knowledge.\n",
    "\n",
    "**Querying & Display**\n",
    "\n",
    "For a given user query, we pulled out relevant chunks and inspected them to confirm what the LLM would see.\n",
    "\n",
    "This ensures transparency and helps debug relevance.\n",
    "\n",
    "**LLM Integration & RAG**\n",
    "\n",
    "We connected a Hugging Face model (flan-t5) via a LangChain QA chain.\n",
    "\n",
    "The retriever feeds only relevant context into the model, reducing hallucinations and keeping answers grounded in the source document.\n",
    "\n",
    "This is exactly where RAG comes into play — retrieval + generation.\n",
    "\n",
    "**Token Limit Handling**\n",
    "\n",
    "We addressed sequence length issues by:\n",
    "\n",
    "Using token-based chunking (not just characters).\n",
    "\n",
    "Keeping chunk size small (≈300–400 tokens).\n",
    "\n",
    "Trying map_reduce chain instead of stuff.\n",
    "\n",
    "Ensuring retrieved context fits within the model’s 512-token window.\n",
    "\n",
    "**Final Takeaway**\n",
    "\n",
    "This pipeline demonstrates how traditional information retrieval (IR) and modern LLMs can be combined into a powerful question-answering system:\n",
    "\n",
    "ChromaDB handles efficient storage & retrieval of document knowledge.\n",
    "\n",
    "Hugging Face embeddings let us find semantically similar text, not just keyword matches.\n",
    "\n",
    "FLAN-T5 (via LangChain) provides natural language understanding & generation.\n",
    "\n",
    "RAG ensures responses are grounded in your documents — accurate, explainable, and reliable.\n",
    "\n",
    "In short: we built a search-augmented LLM that can answer questions from a PDF (or any dataset) without retraining a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad497bf-2e09-4930-a1ed-cfe2ae6c8430",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
